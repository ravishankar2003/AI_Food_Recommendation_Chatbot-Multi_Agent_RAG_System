{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrNTcoiTIiSO",
        "outputId": "ee7964d7-0b22-480c-8bbb-3df3d0a3c512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ All libraries imported successfully\n",
            "âœ“ User Clustering Agent initialized\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Import all required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Persistence\n",
        "import joblib\n",
        "\n",
        "# Visualization (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ“ All libraries imported successfully\")\n",
        "print(\"âœ“ User Clustering Agent initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load data and perform initial exploration\n",
        "def load_and_explore_data(file_path=\"user_data_vectors.xlsx\", sheet_name=\"user_vectors\"):\n",
        "    \"\"\"\n",
        "    Load the Excel file and perform initial data exploration\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load data\n",
        "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "\n",
        "        print(f\"âœ“ Data loaded successfully\")\n",
        "        print(f\"  - Shape: {df.shape}\")\n",
        "        print(f\"  - Users: {df.shape[0]}\")\n",
        "        print(f\"  - Features: {df.shape[1]}\")\n",
        "\n",
        "        # Display column information\n",
        "        print(f\"\\nğŸ“Š Column Information:\")\n",
        "        print(f\"  - Columns: {list(df.columns)}\")\n",
        "\n",
        "        # Check for missing values\n",
        "        missing_values = df.isnull().sum()\n",
        "        if missing_values.any():\n",
        "            print(f\"\\nâš ï¸  Missing values found:\")\n",
        "            print(missing_values[missing_values > 0])\n",
        "        else:\n",
        "            print(f\"\\nâœ“ No missing values detected\")\n",
        "\n",
        "        # Basic statistics\n",
        "        print(f\"\\nğŸ“ˆ Basic Statistics:\")\n",
        "        print(df.describe().round(3))\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading data: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Load the data\n",
        "df_raw = load_and_explore_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aZajs-qRRno",
        "outputId": "5926f248-cf73-4d53-dae7-1b1af24f6178"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Data loaded successfully\n",
            "  - Shape: (327, 13)\n",
            "  - Users: 327\n",
            "  - Features: 13\n",
            "\n",
            "ğŸ“Š Column Information:\n",
            "  - Columns: ['User_ID', 'Age', 'Monthly_Income', 'Average_Rating', 'Average_Z_Score', 'Average_Cost', 'Gender', 'weather', 'Marital_Status', 'C_Type', 'purchase_sensitivity', 'Veg_Ratio', 'Non_Veg_Ratio']\n",
            "\n",
            "âœ“ No missing values detected\n",
            "\n",
            "ğŸ“ˆ Basic Statistics:\n",
            "       User_ID      Age  Monthly_Income  Average_Rating  Average_Z_Score  \\\n",
            "count  327.000  327.000         327.000         327.000          327.000   \n",
            "mean   164.000   -0.086           0.041           4.006           -0.051   \n",
            "std     94.541    0.777           0.949           0.413            0.664   \n",
            "min      1.000   -2.501          -1.205           3.000           -2.151   \n",
            "25%     82.500   -0.531          -0.731           3.780           -0.361   \n",
            "50%    164.000   -0.039          -0.472           4.000            0.012   \n",
            "75%    245.500    0.207           0.908           4.250            0.281   \n",
            "max    327.000    4.392           2.848           5.000            1.943   \n",
            "\n",
            "       Average_Cost   Gender  weather  Marital_Status   C_Type  \\\n",
            "count       327.000  327.000  327.000         327.000  327.000   \n",
            "mean         -0.129    0.440    1.183           0.440    7.144   \n",
            "std           1.687    0.497    0.961           0.497    4.338   \n",
            "min          -5.468    0.000    0.000           0.000    0.000   \n",
            "25%          -0.917    0.000    0.000           0.000    4.000   \n",
            "50%           0.031    0.000    2.000           0.000    6.000   \n",
            "75%           0.714    1.000    2.000           1.000   11.000   \n",
            "max           4.940    1.000    2.000           1.000   15.000   \n",
            "\n",
            "       purchase_sensitivity  Veg_Ratio  Non_Veg_Ratio  \n",
            "count               327.000    327.000        327.000  \n",
            "mean                  2.801      0.555          0.445  \n",
            "std                   0.709      0.292          0.292  \n",
            "min                   0.000      0.000          0.000  \n",
            "25%                   3.000      0.333          0.236  \n",
            "50%                   3.000      0.556          0.444  \n",
            "75%                   3.000      0.764          0.667  \n",
            "max                   4.000      1.000          1.000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Feature selection and column exclusion\n",
        "def prepare_features(df):\n",
        "    \"\"\"\n",
        "    Prepare features by dropping specified columns and organizing remaining features\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying original data\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Columns to drop as specified\n",
        "    columns_to_drop = [\n",
        "        'User_ID',                    # Always drop User_ID\n",
        "        'Non_Veg_Ratio',             # Drop one from the collinear pair (keeping Veg_Ratio)\n",
        "        'Average_Cost'               # Drop one from the correlated pair (keeping Average_Z_Score)\n",
        "    ]\n",
        "\n",
        "    print(\"ğŸ”„ Preparing features...\")\n",
        "    print(f\"  - Dropping columns: {columns_to_drop}\")\n",
        "\n",
        "    # Drop specified columns\n",
        "    df_processed = df_processed.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "    # Identify feature types\n",
        "    # Numerical features (already standardized)\n",
        "    numerical_features = ['Age', 'Monthly_Income', 'Average_Rating', 'Average_Z_Score', 'purchase_sensitivity']\n",
        "\n",
        "    # Categorical features (already one-hot encoded as integers)\n",
        "    categorical_features = ['Gender', 'weather', 'Marital_Status', 'C_Type']\n",
        "\n",
        "    # Behavioral features\n",
        "    behavioral_features = ['Veg_Ratio']\n",
        "\n",
        "    # Verify all features exist\n",
        "    all_features = numerical_features + categorical_features + behavioral_features\n",
        "    available_features = [col for col in all_features if col in df_processed.columns]\n",
        "\n",
        "    print(f\"  - Numerical features: {numerical_features}\")\n",
        "    print(f\"  - Categorical features: {categorical_features}\")\n",
        "    print(f\"  - Behavioral features: {behavioral_features}\")\n",
        "    print(f\"  - Total features for clustering: {len(available_features)}\")\n",
        "\n",
        "    # Final feature matrix\n",
        "    X = df_processed[available_features]\n",
        "\n",
        "    print(f\"âœ“ Feature matrix prepared\")\n",
        "    print(f\"  - Shape: {X.shape}\")\n",
        "    print(f\"  - Features: {list(X.columns)}\")\n",
        "\n",
        "    return X, available_features, numerical_features, categorical_features, behavioral_features\n",
        "\n",
        "# Prepare features\n",
        "X, feature_names, num_features, cat_features, behav_features = prepare_features(df_raw)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV7DSWa0RXYh",
        "outputId": "a91348ae-c5c7-4262-ea2b-4265c5729e69"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Preparing features...\n",
            "  - Dropping columns: ['User_ID', 'Non_Veg_Ratio', 'Average_Cost']\n",
            "  - Numerical features: ['Age', 'Monthly_Income', 'Average_Rating', 'Average_Z_Score', 'purchase_sensitivity']\n",
            "  - Categorical features: ['Gender', 'weather', 'Marital_Status', 'C_Type']\n",
            "  - Behavioral features: ['Veg_Ratio']\n",
            "  - Total features for clustering: 10\n",
            "âœ“ Feature matrix prepared\n",
            "  - Shape: (327, 10)\n",
            "  - Features: ['Age', 'Monthly_Income', 'Average_Rating', 'Average_Z_Score', 'purchase_sensitivity', 'Gender', 'weather', 'Marital_Status', 'C_Type', 'Veg_Ratio']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Create preprocessing pipeline\n",
        "def create_preprocessing_pipeline(numerical_features, categorical_features, behavioral_features):\n",
        "    \"\"\"\n",
        "    Create preprocessing pipeline for the features\n",
        "    Since features are already standardized/encoded, we use passthrough\n",
        "    \"\"\"\n",
        "    # Since features are already preprocessed, we use passthrough\n",
        "    # But we still create a pipeline for consistency and future flexibility\n",
        "\n",
        "    # All features get passthrough treatment since they're already processed\n",
        "    all_features = numerical_features + categorical_features + behavioral_features\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('passthrough', 'passthrough', all_features)\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "\n",
        "    print(\"ğŸ”§ Preprocessing pipeline created\")\n",
        "    print(\"  - Strategy: Passthrough (features already preprocessed)\")\n",
        "    print(f\"  - Processing {len(all_features)} features\")\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "preprocessor = create_preprocessing_pipeline(num_features, cat_features, behav_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VSLFdV9Rkry",
        "outputId": "1e479637-37c0-4607-f12d-f7d12d48312f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Preprocessing pipeline created\n",
            "  - Strategy: Passthrough (features already preprocessed)\n",
            "  - Processing 10 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Train K-means++ clustering model\n",
        "def train_kmeans_model(X, preprocessor, n_clusters=4, random_state=42):\n",
        "    \"\"\"\n",
        "    Train K-means++ clustering model\n",
        "    \"\"\"\n",
        "    print(\"ğŸš€ Training K-means++ clustering model...\")\n",
        "\n",
        "    # Create the full pipeline\n",
        "    kmeans_pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('kmeans', KMeans(\n",
        "            n_clusters=n_clusters,\n",
        "            init='k-means++',\n",
        "            n_init=10,\n",
        "            max_iter=300,\n",
        "            random_state=random_state\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    # Fit the model\n",
        "    kmeans_pipeline.fit(X)\n",
        "\n",
        "    # Get cluster labels\n",
        "    labels = kmeans_pipeline.named_steps['kmeans'].labels_\n",
        "\n",
        "    # Get centroids\n",
        "    centroids = kmeans_pipeline.named_steps['kmeans'].cluster_centers_\n",
        "\n",
        "    # Calculate metrics\n",
        "    X_transformed = preprocessor.fit_transform(X)\n",
        "    inertia = kmeans_pipeline.named_steps['kmeans'].inertia_\n",
        "    silhouette_avg = silhouette_score(X_transformed, labels)\n",
        "\n",
        "    print(f\"âœ“ Model training completed\")\n",
        "    print(f\"  - Clusters: {n_clusters}\")\n",
        "    print(f\"  - Inertia: {inertia:.3f}\")\n",
        "    print(f\"  - Silhouette Score: {silhouette_avg:.3f}\")\n",
        "\n",
        "    # Cluster distribution\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    print(f\"  - Cluster distribution: {dict(zip(unique, counts))}\")\n",
        "\n",
        "    return kmeans_pipeline, labels, centroids, inertia, silhouette_avg\n",
        "\n",
        "# Train the model\n",
        "model_pipeline, cluster_labels, centroids, inertia_score, silhouette_score = train_kmeans_model(X, preprocessor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9ZsmZA3RnZg",
        "outputId": "72d49fd3-7413-4350-f5e9-67727244cf20"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Training K-means++ clustering model...\n",
            "âœ“ Model training completed\n",
            "  - Clusters: 4\n",
            "  - Inertia: 1718.219\n",
            "  - Silhouette Score: 0.279\n",
            "  - Cluster distribution: {np.int32(0): np.int64(80), np.int32(1): np.int64(66), np.int32(2): np.int64(106), np.int32(3): np.int64(75)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Generate persona metadata\n",
        "def generate_persona_metadata(X, cluster_labels, centroids, feature_names):\n",
        "    \"\"\"\n",
        "    Generate persona metadata for each cluster\n",
        "    \"\"\"\n",
        "    print(\"ğŸ‘¥ Generating persona metadata...\")\n",
        "\n",
        "    # Define persona names (can be customized based on business context)\n",
        "    persona_names = {\n",
        "        0: \"Established Urban Professionals\",\n",
        "        1: \"Premium Self-Employed Segment\",\n",
        "        2: \"Young Urban Students\",\n",
        "        3: \"Price-Sensitive Employees\"\n",
        "    }\n",
        "\n",
        "    # Create DataFrame with features and labels\n",
        "    df_with_labels = X.copy()\n",
        "    df_with_labels['cluster_id'] = cluster_labels\n",
        "\n",
        "    metadata = []\n",
        "\n",
        "    for cluster_id in range(4):\n",
        "        cluster_data = df_with_labels[df_with_labels['cluster_id'] == cluster_id]\n",
        "\n",
        "        # Calculate means for numerical features\n",
        "        cluster_means = cluster_data.drop('cluster_id', axis=1).mean()\n",
        "\n",
        "        # Create metadata structure\n",
        "        cluster_metadata = {\n",
        "            'cluster_id': int(cluster_id),\n",
        "            'persona_name': persona_names.get(cluster_id, f\"Cluster_{cluster_id}\"),\n",
        "            'cluster_size': len(cluster_data),\n",
        "            'percentage_of_total': round(len(cluster_data) / len(X) * 100, 2)\n",
        "        }\n",
        "\n",
        "        # Add feature averages\n",
        "        for feature in feature_names:\n",
        "            if feature in cluster_means.index:\n",
        "                cluster_metadata[f'avg_{feature}'] = round(cluster_means[feature], 4)\n",
        "\n",
        "        # Add categorical distributions for specific features\n",
        "        categorical_features = ['Gender', 'weather', 'Marital_Status', 'C_Type']\n",
        "        for cat_feature in categorical_features:\n",
        "            if cat_feature in cluster_data.columns:\n",
        "                # Calculate distribution\n",
        "                value_counts = cluster_data[cat_feature].value_counts()\n",
        "                total = len(cluster_data)\n",
        "                distribution = {str(k): round(v/total, 4) for k, v in value_counts.items()}\n",
        "                cluster_metadata[f'{cat_feature}_distribution'] = distribution\n",
        "\n",
        "        metadata.append(cluster_metadata)\n",
        "\n",
        "    print(\"âœ“ Persona metadata generated\")\n",
        "    for i, meta in enumerate(metadata):\n",
        "        print(f\"  - Cluster {i}: {meta['persona_name']} ({meta['cluster_size']} users, {meta['percentage_of_total']}%)\")\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# Generate metadata\n",
        "persona_metadata = generate_persona_metadata(X, cluster_labels, centroids, feature_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl5dxvQaRpD-",
        "outputId": "0d640fd6-a727-4f9c-bbb3-1a7677cba846"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ‘¥ Generating persona metadata...\n",
            "âœ“ Persona metadata generated\n",
            "  - Cluster 0: Established Urban Professionals (80 users, 24.46%)\n",
            "  - Cluster 1: Premium Self-Employed Segment (66 users, 20.18%)\n",
            "  - Cluster 2: Young Urban Students (106 users, 32.42%)\n",
            "  - Cluster 3: Price-Sensitive Employees (75 users, 22.94%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Create output generation functions\n",
        "def generate_cluster_labels_csv(cluster_labels):\n",
        "    \"\"\"\n",
        "    Generate cluster labels CSV file\n",
        "    \"\"\"\n",
        "    labels_df = pd.DataFrame({\n",
        "        'row_index': range(len(cluster_labels)),\n",
        "        'cluster_id': cluster_labels\n",
        "    })\n",
        "\n",
        "    labels_df.to_csv('cluster_labels.csv', index=False)\n",
        "    print(\"âœ“ cluster_labels.csv generated\")\n",
        "    return labels_df\n",
        "\n",
        "def generate_cluster_metadata_json(metadata):\n",
        "    \"\"\"\n",
        "    Generate cluster metadata JSON file\n",
        "    \"\"\"\n",
        "    with open('cluster_metadata.json', 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    print(\"âœ“ cluster_metadata.json generated\")\n",
        "\n",
        "    # Also create CSV version\n",
        "    metadata_df = pd.DataFrame(metadata)\n",
        "    metadata_df.to_csv('cluster_metadata.csv', index=False)\n",
        "    print(\"âœ“ cluster_metadata.csv generated\")\n",
        "    return metadata_df\n",
        "\n",
        "def generate_cluster_preferences_csv(centroids, feature_names):\n",
        "    \"\"\"\n",
        "    Generate cluster preferences CSV file\n",
        "    \"\"\"\n",
        "    preferences_df = pd.DataFrame(centroids, columns=feature_names)\n",
        "    preferences_df.insert(0, 'cluster_id', range(len(centroids)))\n",
        "\n",
        "    preferences_df.to_csv('cluster_preferences.csv', index=False)\n",
        "    print(\"âœ“ cluster_preferences.csv generated\")\n",
        "    return preferences_df\n",
        "\n",
        "def create_download_package():\n",
        "    \"\"\"\n",
        "    Create ZIP package with all outputs\n",
        "    \"\"\"\n",
        "    zip_filename = f'user_clustering_outputs_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.zip'\n",
        "\n",
        "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "        files_to_zip = [\n",
        "            'cluster_labels.csv',\n",
        "            'cluster_metadata.json',\n",
        "            'cluster_metadata.csv',\n",
        "            'cluster_preferences.csv'\n",
        "        ]\n",
        "\n",
        "        for file in files_to_zip:\n",
        "            if os.path.exists(file):\n",
        "                zipf.write(file)\n",
        "\n",
        "    print(f\"âœ“ Download package created: {zip_filename}\")\n",
        "    return zip_filename\n",
        "\n",
        "# Generate all outputs\n",
        "labels_df = generate_cluster_labels_csv(cluster_labels)\n",
        "metadata_df = generate_cluster_metadata_json(persona_metadata)\n",
        "preferences_df = generate_cluster_preferences_csv(centroids, feature_names)\n",
        "zip_file = create_download_package()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC4kXbMrRupi",
        "outputId": "45cd1143-0227-4f0f-cee5-9a2026679bcd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ cluster_labels.csv generated\n",
            "âœ“ cluster_metadata.json generated\n",
            "âœ“ cluster_metadata.csv generated\n",
            "âœ“ cluster_preferences.csv generated\n",
            "âœ“ Download package created: user_clustering_outputs_20250714_161308.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Save model for future inference\n",
        "def save_model_pipeline(pipeline, feature_names, filename='user_clustering_agent.pkl'):\n",
        "    \"\"\"\n",
        "    Save the trained pipeline for future inference\n",
        "    \"\"\"\n",
        "    model_package = {\n",
        "        'pipeline': pipeline,\n",
        "        'feature_names': feature_names,\n",
        "        'training_date': datetime.now().isoformat(),\n",
        "        'model_info': {\n",
        "            'n_clusters': 4,\n",
        "            'algorithm': 'k-means++',\n",
        "            'random_state': 42\n",
        "        }\n",
        "    }\n",
        "\n",
        "    joblib.dump(model_package, filename)\n",
        "    print(f\"âœ“ Model saved to {filename}\")\n",
        "    return filename\n",
        "\n",
        "# Save the model\n",
        "model_file = save_model_pipeline(model_pipeline, feature_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UdVBLLBRxVs",
        "outputId": "72c9718a-9865-4362-856f-c212d509683e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Model saved to user_clustering_agent.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Create inference function for new users\n",
        "def load_model_and_predict(new_user_data, model_path='user_clustering_agent.pkl'):\n",
        "    \"\"\"\n",
        "    Load saved model and predict cluster for new user\n",
        "\n",
        "    Parameters:\n",
        "    new_user_data: dict with keys matching the feature names\n",
        "    model_path: path to saved model file\n",
        "\n",
        "    Returns:\n",
        "    cluster_id: int, assigned cluster\n",
        "    distances: array, distances to all centroids\n",
        "    confidence: float, confidence score (inverse of min distance)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load model package\n",
        "        model_package = joblib.load(model_path)\n",
        "        pipeline = model_package['pipeline']\n",
        "        feature_names = model_package['feature_names']\n",
        "\n",
        "        # Convert input to DataFrame\n",
        "        user_df = pd.DataFrame([new_user_data])\n",
        "\n",
        "        # Ensure all required features are present\n",
        "        for feature in feature_names:\n",
        "            if feature not in user_df.columns:\n",
        "                user_df[feature] = 0.0  # Default value for missing features\n",
        "\n",
        "        # Reorder columns to match training\n",
        "        user_df = user_df[feature_names]\n",
        "\n",
        "        # Predict cluster\n",
        "        cluster_id = pipeline.predict(user_df)[0]\n",
        "\n",
        "        # Get distances to all centroids\n",
        "        distances = pipeline.named_steps['kmeans'].transform(\n",
        "            pipeline.named_steps['preprocessor'].transform(user_df)\n",
        "        )[0]\n",
        "\n",
        "        # Calculate confidence (inverse of minimum distance)\n",
        "        confidence = 1 / (1 + min(distances))\n",
        "\n",
        "        return int(cluster_id), distances, confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error in prediction: {str(e)}\")\n",
        "        return None, None, None\n",
        "\n",
        "# Test the inference function\n",
        "test_user = {\n",
        "    'Age': 0.5,\n",
        "    'Monthly_Income': 0.2,\n",
        "    'Average_Rating': 4.2,\n",
        "    'Average_Z_Score': 0.1,\n",
        "    'purchase_sensitivity': 2,\n",
        "    'Gender': 1,\n",
        "    'weather': 1,\n",
        "    'Marital_Status': 0,\n",
        "    'C_Type': 5,\n",
        "    'Veg_Ratio': 0.8\n",
        "}\n",
        "\n",
        "predicted_cluster, distances, confidence = load_model_and_predict(test_user)\n",
        "print(f\"ğŸ”® Test Prediction:\")\n",
        "print(f\"  - Cluster: {predicted_cluster}\")\n",
        "print(f\"  - Distances: {distances.round(3)}\")\n",
        "print(f\"  - Confidence: {confidence:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LL3yvHhRzYu",
        "outputId": "e5a221db-a3e7-4d04-d995-196fa91d7e18"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”® Test Prediction:\n",
            "  - Cluster: 2\n",
            "  - Distances: [3.463 8.533 1.205 5.02 ]\n",
            "  - Confidence: 0.454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Validation and quality checks\n",
        "def validate_clustering_results(X, labels, centroids, silhouette_avg):\n",
        "    \"\"\"\n",
        "    Perform validation and quality checks\n",
        "    \"\"\"\n",
        "    print(\"ğŸ” Validation and Quality Checks:\")\n",
        "\n",
        "    # Check 1: Verify excluded columns are not in outputs\n",
        "    excluded_columns = ['User_ID', 'Non_Veg_Ratio', 'Average_Cost']\n",
        "    print(f\"  âœ“ Excluded columns verification:\")\n",
        "    print(f\"    - Excluded: {excluded_columns}\")\n",
        "    print(f\"    - Present features: {list(X.columns)}\")\n",
        "\n",
        "    # Check 2: Silhouette score validation\n",
        "    print(f\"  âœ“ Silhouette score: {silhouette_avg:.3f}\")\n",
        "    if silhouette_avg > 0:\n",
        "        print(f\"    - Status: âœ“ Good (> 0)\")\n",
        "    else:\n",
        "        print(f\"    - Status: âš ï¸ Needs improvement\")\n",
        "\n",
        "    # Check 3: Cluster balance\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    cluster_balance = dict(zip(unique, counts))\n",
        "    print(f\"  âœ“ Cluster balance: {cluster_balance}\")\n",
        "\n",
        "    min_cluster_size = min(counts)\n",
        "    max_cluster_size = max(counts)\n",
        "    balance_ratio = min_cluster_size / max_cluster_size\n",
        "\n",
        "    if balance_ratio > 0.1:  # At least 10% of largest cluster\n",
        "        print(f\"    - Status: âœ“ Well balanced (ratio: {balance_ratio:.3f})\")\n",
        "    else:\n",
        "        print(f\"    - Status: âš ï¸ Imbalanced (ratio: {balance_ratio:.3f})\")\n",
        "\n",
        "    # Check 4: Centroid interpretability\n",
        "    print(f\"  âœ“ Centroid analysis:\")\n",
        "    for i, centroid in enumerate(centroids):\n",
        "        print(f\"    - Cluster {i}: mean={centroid.mean():.3f}, std={centroid.std():.3f}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# Run validation\n",
        "validation_passed = validate_clustering_results(X, cluster_labels, centroids, silhouette_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yb0YyEsR1ny",
        "outputId": "52cd312f-7b84-41d2-e86a-a80b8ac88ba1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Validation and Quality Checks:\n",
            "  âœ“ Excluded columns verification:\n",
            "    - Excluded: ['User_ID', 'Non_Veg_Ratio', 'Average_Cost']\n",
            "    - Present features: ['Age', 'Monthly_Income', 'Average_Rating', 'Average_Z_Score', 'purchase_sensitivity', 'Gender', 'weather', 'Marital_Status', 'C_Type', 'Veg_Ratio']\n",
            "  âœ“ Silhouette score: 0.279\n",
            "    - Status: âœ“ Good (> 0)\n",
            "  âœ“ Cluster balance: {np.int32(0): np.int64(80), np.int32(1): np.int64(66), np.int32(2): np.int64(106), np.int32(3): np.int64(75)}\n",
            "    - Status: âœ“ Well balanced (ratio: 0.623)\n",
            "  âœ“ Centroid analysis:\n",
            "    - Cluster 0: mean=1.134, std=1.256\n",
            "    - Cluster 1: mean=2.275, std=3.942\n",
            "    - Cluster 2: mean=1.463, std=1.820\n",
            "    - Cluster 3: mean=1.902, std=2.941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Retraining utility function\n",
        "def retrain_clustering_agent(file_path=\"user_data_vectors.xlsx\",\n",
        "                           sheet_name=\"user_vectors\",\n",
        "                           n_clusters=4,\n",
        "                           random_state=42):\n",
        "    \"\"\"\n",
        "    Utility function to retrain the clustering agent with fresh data\n",
        "    \"\"\"\n",
        "    print(\"ğŸ”„ Retraining User Clustering Agent...\")\n",
        "\n",
        "    # Load fresh data\n",
        "    df_fresh = load_and_explore_data(file_path, sheet_name)\n",
        "    if df_fresh is None:\n",
        "        return False\n",
        "\n",
        "    # Prepare features\n",
        "    X_fresh, feature_names_fresh, num_feat, cat_feat, behav_feat = prepare_features(df_fresh)\n",
        "\n",
        "    # Create new preprocessing pipeline\n",
        "    preprocessor_fresh = create_preprocessing_pipeline(num_feat, cat_feat, behav_feat)\n",
        "\n",
        "    # Train new model\n",
        "    model_fresh, labels_fresh, centroids_fresh, inertia_fresh, silhouette_fresh = train_kmeans_model(\n",
        "        X_fresh, preprocessor_fresh, n_clusters, random_state\n",
        "    )\n",
        "\n",
        "    # Generate new metadata\n",
        "    metadata_fresh = generate_persona_metadata(X_fresh, labels_fresh, centroids_fresh, feature_names_fresh)\n",
        "\n",
        "    # Generate new outputs\n",
        "    generate_cluster_labels_csv(labels_fresh)\n",
        "    generate_cluster_metadata_json(metadata_fresh)\n",
        "    generate_cluster_preferences_csv(centroids_fresh, feature_names_fresh)\n",
        "\n",
        "    # Save new model\n",
        "    save_model_pipeline(model_fresh, feature_names_fresh)\n",
        "\n",
        "    # Create new download package\n",
        "    new_zip = create_download_package()\n",
        "\n",
        "    print(\"âœ“ Retraining completed successfully\")\n",
        "    return True\n",
        "\n",
        "# The retraining function is now available for use\n",
        "print(\"ğŸ”§ Retraining utility function created\")\n",
        "print(\"  - Usage: retrain_clustering_agent()\")\n",
        "print(\"  - Will reload data, retrain model, and generate new outputs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruwm-e4IR429",
        "outputId": "1ec5d27c-4e9d-4c84-87b5-b8a943e0c365"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Retraining utility function created\n",
            "  - Usage: retrain_clustering_agent()\n",
            "  - Will reload data, retrain model, and generate new outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Display final results and summary\n",
        "def display_final_summary():\n",
        "    \"\"\"\n",
        "    Display comprehensive summary of the clustering results\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"ğŸ¯ USER CLUSTERING AGENT - FINAL RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"\\nğŸ“Š DATASET SUMMARY:\")\n",
        "    print(f\"  - Total users clustered: {len(cluster_labels)}\")\n",
        "    print(f\"  - Features used: {len(feature_names)}\")\n",
        "    print(f\"  - Clusters created: 4\")\n",
        "\n",
        "    print(f\"\\nğŸ¯ MODEL PERFORMANCE:\")\n",
        "    print(f\"  - Inertia: {inertia_score:.3f}\")\n",
        "    print(f\"  - Silhouette Score: {silhouette_score:.3f}\")\n",
        "    print(f\"  - Algorithm: K-means++\")\n",
        "\n",
        "    print(f\"\\nğŸ‘¥ CLUSTER DISTRIBUTION:\")\n",
        "    for i, meta in enumerate(persona_metadata):\n",
        "        print(f\"  - Cluster {i}: {meta['persona_name']}\")\n",
        "        print(f\"    â””â”€ Users: {meta['cluster_size']} ({meta['percentage_of_total']}%)\")\n",
        "\n",
        "    print(f\"\\nğŸ“ GENERATED FILES:\")\n",
        "    files = [\n",
        "        'cluster_labels.csv',\n",
        "        'cluster_metadata.json',\n",
        "        'cluster_metadata.csv',\n",
        "        'cluster_preferences.csv',\n",
        "        'user_clustering_agent.pkl',\n",
        "        zip_file\n",
        "    ]\n",
        "\n",
        "    for file in files:\n",
        "        if os.path.exists(file):\n",
        "            size = os.path.getsize(file)\n",
        "            print(f\"  âœ“ {file} ({size} bytes)\")\n",
        "\n",
        "    print(f\"\\nğŸš€ READY FOR DEPLOYMENT:\")\n",
        "    print(f\"  - Model saved and ready for inference\")\n",
        "    print(f\"  - Download package created: {zip_file}\")\n",
        "    print(f\"  - Retraining function available\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"âœ… USER CLUSTERING AGENT IMPLEMENTATION COMPLETE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# Display final summary\n",
        "display_final_summary()\n"
      ],
      "metadata": {
        "id": "CoKRVjlrR_zu",
        "outputId": "06ca2eac-2deb-4aaf-c7bb-09944d2ef71b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ¯ USER CLUSTERING AGENT - FINAL RESULTS\n",
            "============================================================\n",
            "\n",
            "ğŸ“Š DATASET SUMMARY:\n",
            "  - Total users clustered: 327\n",
            "  - Features used: 10\n",
            "  - Clusters created: 4\n",
            "\n",
            "ğŸ¯ MODEL PERFORMANCE:\n",
            "  - Inertia: 1718.219\n",
            "  - Silhouette Score: 0.279\n",
            "  - Algorithm: K-means++\n",
            "\n",
            "ğŸ‘¥ CLUSTER DISTRIBUTION:\n",
            "  - Cluster 0: Established Urban Professionals\n",
            "    â””â”€ Users: 80 (24.46%)\n",
            "  - Cluster 1: Premium Self-Employed Segment\n",
            "    â””â”€ Users: 66 (20.18%)\n",
            "  - Cluster 2: Young Urban Students\n",
            "    â””â”€ Users: 106 (32.42%)\n",
            "  - Cluster 3: Price-Sensitive Employees\n",
            "    â””â”€ Users: 75 (22.94%)\n",
            "\n",
            "ğŸ“ GENERATED FILES:\n",
            "  âœ“ cluster_labels.csv (1873 bytes)\n",
            "  âœ“ cluster_metadata.json (3195 bytes)\n",
            "  âœ“ cluster_metadata.csv (1337 bytes)\n",
            "  âœ“ cluster_preferences.csv (859 bytes)\n",
            "  âœ“ user_clustering_agent.pkl (4306 bytes)\n",
            "  âœ“ user_clustering_outputs_20250714_161308.zip (7754 bytes)\n",
            "\n",
            "ğŸš€ READY FOR DEPLOYMENT:\n",
            "  - Model saved and ready for inference\n",
            "  - Download package created: user_clustering_outputs_20250714_161308.zip\n",
            "  - Retraining function available\n",
            "\n",
            "============================================================\n",
            "âœ… USER CLUSTERING AGENT IMPLEMENTATION COMPLETE\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vnIpfPpkSB4q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}